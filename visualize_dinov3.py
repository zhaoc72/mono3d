#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆ DINOv3 å¯è§†åŒ–è„šæœ¬
æ–°å¢ï¼šæ£€æµ‹æ¡†å’Œåˆ†å‰²ç»“æœçš„å¯è§†åŒ–ä¿å­˜
"""

import argparse
import sys
from pathlib import Path
import cv2
import numpy as np
import torch
import yaml

def load_yaml(path: str):
    """åŠ è½½ YAML é…ç½®æ–‡ä»¶"""
    with open(path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def normalize_to_uint8(data: np.ndarray) -> np.ndarray:
    """å°†æµ®ç‚¹æ•°ç»„å½’ä¸€åŒ–åˆ° uint8 èŒƒå›´ [0, 255]"""
    finite = np.isfinite(data)
    if not finite.any():
        return np.zeros_like(data, dtype=np.uint8)
    
    clipped = np.zeros_like(data, dtype=np.float16)
    clipped[finite] = data[finite]
    minimum = float(clipped[finite].min())
    maximum = float(clipped[finite].max())
    
    if maximum - minimum < 1e-6:
        return np.zeros_like(clipped, dtype=np.uint8)
    
    normalized = (clipped - minimum) / (maximum - minimum)
    normalized = np.clip(normalized, 0.0, 1.0)
    return (normalized * 255).astype(np.uint8)

def save_heatmap(map_2d: np.ndarray, output_path: Path, base_image: np.ndarray = None):
    """ä¿å­˜çƒ­åŠ›å›¾å¯è§†åŒ–"""
    heatmap_uint8 = normalize_to_uint8(map_2d)
    heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_VIRIDIS)
    
    # ä¿å­˜ç‹¬ç«‹çƒ­åŠ›å›¾
    cv2.imwrite(str(output_path.with_suffix('.png')), heatmap_color)
    print(f"   âœ… Saved: {output_path.name}")
    
    # å¦‚æœæä¾›äº†åŸºç¡€å›¾åƒï¼Œä¿å­˜å åŠ å›¾
    if base_image is not None:
        base_bgr = cv2.cvtColor(base_image, cv2.COLOR_RGB2BGR)
        # è°ƒæ•´çƒ­åŠ›å›¾å¤§å°ä»¥åŒ¹é…å›¾åƒ
        if heatmap_color.shape[:2] != base_bgr.shape[:2]:
            heatmap_color = cv2.resize(
                heatmap_color,
                (base_bgr.shape[1], base_bgr.shape[0]),
                interpolation=cv2.INTER_CUBIC
            )
        overlay = cv2.addWeighted(base_bgr, 0.6, heatmap_color, 0.4, 0.0)
        overlay_path = output_path.with_name(output_path.stem + '_overlay.png')
        cv2.imwrite(str(overlay_path), overlay)
        print(f"   âœ… Saved: {overlay_path.name}")

def save_detection_visualization(detection, image_rgb, processed_shape, output_dir):
    """ä¿å­˜æ£€æµ‹æ¡†å¯è§†åŒ–"""
    print(f"\nğŸ’¾ Saving detection visualization...")
    det_vis_bgr = cv2.cvtColor(image_rgb.copy(), cv2.COLOR_RGB2BGR)
    
    # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
    scale_x = image_rgb.shape[1] / processed_shape[1]
    scale_y = image_rgb.shape[0] / processed_shape[0]
    
    # å®šä¹‰é¢œè‰²è°ƒè‰²æ¿
    colors = [
        (240, 86, 60), (67, 160, 71), (66, 133, 244), 
        (171, 71, 188), (255, 202, 40), (38, 198, 218),
        (255, 112, 67), (124, 179, 66)
    ]
    
    for idx, (box, score, class_id) in enumerate(zip(
        detection.boxes, detection.scores, detection.class_ids
    )):
        # ç¼©æ”¾æ¡†åæ ‡
        x1 = int(box[0] * scale_x)
        y1 = int(box[1] * scale_y)
        x2 = int(box[2] * scale_x)
        y2 = int(box[3] * scale_y)
        
        # è£å‰ªåˆ°å›¾åƒè¾¹ç•Œ
        x1 = max(0, min(det_vis_bgr.shape[1] - 1, x1))
        y1 = max(0, min(det_vis_bgr.shape[0] - 1, y1))
        x2 = max(0, min(det_vis_bgr.shape[1] - 1, x2))
        y2 = max(0, min(det_vis_bgr.shape[0] - 1, y2))
        
        # é€‰æ‹©é¢œè‰²
        color = colors[idx % len(colors)]
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        cv2.rectangle(det_vis_bgr, (x1, y1), (x2, y2), color, 2)
        
        # æ·»åŠ æ ‡ç­¾
        class_name = detection.class_names[int(class_id)] if detection.class_names else str(int(class_id))
        label = f"{class_name}:{score:.2f}"
        cv2.putText(
            det_vis_bgr, label, (x1, max(15, y1 - 5)),
            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA
        )
    
    detection_path = output_dir / "detections.png"
    cv2.imwrite(str(detection_path), det_vis_bgr)
    print(f"   âœ… Saved: {detection_path.name}")

def save_segmentation_visualization(segmentation, image_rgb, processed_shape, output_dir):
    """ä¿å­˜åˆ†å‰²ç»“æœå¯è§†åŒ–"""
    print(f"\nğŸ’¾ Saving segmentation visualization...")
    
    # è®¡ç®—æ¦‚ç‡ï¼ˆsigmoidæ¿€æ´»ï¼‰
    logits = segmentation.logits.astype(np.float32)
    probs = 1.0 / (1.0 + np.exp(-logits))
    
    # è·å–æœ€å¤§æ¦‚ç‡çš„ç±»åˆ«
    class_indices = probs.argmax(axis=0)
    class_scores = probs.max(axis=0)
    
    # åˆ›å»ºå½©è‰²åˆ†å‰²å›¾ï¼ˆä½¿ç”¨éšæœºé¢œè‰²ï¼‰
    np.random.seed(42)
    palette = np.random.randint(0, 255, size=(150, 3), dtype=np.uint8)
    palette[0] = [0, 0, 0]  # èƒŒæ™¯ä¸ºé»‘è‰²
    
    color_map = palette[class_indices].astype(np.uint8)
    
    # è°ƒæ•´å¤§å°åˆ°åŸå§‹å›¾åƒå°ºå¯¸
    if color_map.shape[:2] != image_rgb.shape[:2]:
        color_map = cv2.resize(
            color_map,
            (image_rgb.shape[1], image_rgb.shape[0]),
            interpolation=cv2.INTER_NEAREST
        )
    
    # ä¿å­˜åˆ†å‰²å›¾
    seg_path = output_dir / "segmentation_map.png"
    cv2.imwrite(str(seg_path), cv2.cvtColor(color_map, cv2.COLOR_RGB2BGR))
    print(f"   âœ… Saved: {seg_path.name}")
    
    # ä¿å­˜å åŠ å›¾
    base_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
    color_bgr = cv2.cvtColor(color_map, cv2.COLOR_RGB2BGR)
    overlay = cv2.addWeighted(base_bgr, 0.6, color_bgr, 0.4, 0.0)
    overlay_path = output_dir / "segmentation_overlay.png"
    cv2.imwrite(str(overlay_path), overlay)
    print(f"   âœ… Saved: {overlay_path.name}")
    
    # ä¿å­˜å‰æ™¯æ¦‚ç‡çƒ­åŠ›å›¾
    foreground_prob = class_scores
    save_heatmap(foreground_prob, output_dir / "segmentation_foreground", image_rgb)

def main():
    parser = argparse.ArgumentParser(
        description="å¯è§†åŒ– DINOv3 backbone å’Œ adapter è¾“å‡º"
    )
    parser.add_argument("--input", required=True, help="è¾“å…¥å›¾åƒè·¯å¾„")
    parser.add_argument("--output", required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--config", required=True, help="é…ç½® YAML æ–‡ä»¶")
    parser.add_argument("--device", default="cuda", help="è®¾å¤‡ (cuda/cpu)")
    parser.add_argument(
        "--detection-threshold", type=float, default=0.05,
        help="æ£€æµ‹åˆ†æ•°é˜ˆå€¼ (é»˜è®¤: 0.05)"
    )
    parser.add_argument(
        "--skip-adapters", action="store_true",
        help="è·³è¿‡ adapterï¼Œåªè¿è¡Œ backbone"
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    print("=" * 70)
    print("DINOv3 Visualization (ViT-7B/16) - Fixed Version")
    print("=" * 70)
    print(f"\nğŸ“– Loading config: {args.config}")
    
    config = load_yaml(args.config)
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½å›¾åƒ
    print(f"\nğŸ–¼ï¸  Loading image: {args.input}")
    image_bgr = cv2.imread(args.input)
    if image_bgr is None:
        raise ValueError(f"Failed to load image from {args.input}")
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    print(f"   Image shape: {image_rgb.shape}")
    
    # å¯¼å…¥å¿…éœ€æ¨¡å—
    print(f"\nğŸ“¦ Importing modules...")
    sys.path.insert(0, str(Path(__file__).parent))
    
    from src.config import Dinov3BackboneConfig
    from src.dinov3_feature import Dinov3Backbone
    
    # è®¾ç½®è®¾å¤‡å’Œæ•°æ®ç±»å‹
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    dtype = torch.float16
    print(f"   Device: {device}")
    print(f"   Dtype: {dtype}")
    
    # è§£æ DINOv3 backbone é…ç½®
    print(f"\nâš™ï¸  Parsing configuration...")
    model_section = config.get('model', config)
    backbone_config_dict = model_section.get('backbone', {})
    
    if not backbone_config_dict:
        raise ValueError("Could not find 'model.backbone' section in config")
    
    print(f"   Backbone config keys: {list(backbone_config_dict.keys())}")
    
    # åˆå§‹åŒ– DINOv3 backbone
    print(f"\n============================================================")
    print(f"ğŸ§  Step 1: Initializing DINOv3 Backbone")
    print(f"============================================================")
    
    repo_path = backbone_config_dict.get('repo_path')
    checkpoint_path = backbone_config_dict.get('checkpoint_path')
    
    print(f"   Repo path: {repo_path}")
    print(f"   Checkpoint: {checkpoint_path}")
    print(f"   Model name: {backbone_config_dict.get('model_name', 'dinov3_vit7b16')}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if repo_path and not Path(repo_path).exists():
        raise FileNotFoundError(f"DINOv3 repo not found: {repo_path}")
    if checkpoint_path and not Path(checkpoint_path).exists():
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    dinov3_config = Dinov3BackboneConfig(
        repo_path=repo_path,
        model_name=backbone_config_dict.get('model_name', 'dinov3_vit7b16'),
        checkpoint_path=checkpoint_path,
        image_size=backbone_config_dict.get('image_size', 518),
        output_layers=tuple(backbone_config_dict.get('output_layers', [9, 19, 29, 39])),
        enable_objectness=backbone_config_dict.get('enable_objectness', True),
        enable_pca=backbone_config_dict.get('enable_pca', True),
        pca_dim=backbone_config_dict.get('pca_dim', 32),
        patch_size=backbone_config_dict.get('patch_size', 16),
    )
    
    backbone = Dinov3Backbone(dinov3_config, device=device, dtype=dtype)
    print(f"   âœ… Backbone initialized successfully")
    
    # æå–ç‰¹å¾
    print(f"\nğŸ”„ Extracting DINOv3 features...")
    features = backbone.extract_features(image_rgb)
    
    patch_tokens = features['patch_tokens']
    patch_tokens_raw = features.get('patch_tokens_raw', patch_tokens)
    grid_size = features['grid_size']
    processed_shape = features['processed_image_shape']
    objectness_map = features.get('objectness_map')
    attention_map = features.get('attention_map')
    
    print(f"   âœ… Feature extraction complete:")
    print(f"      - Grid size: {grid_size}")
    print(f"      - Processed shape: {processed_shape}")
    print(f"      - Patch tokens: {patch_tokens.shape}")
    print(f"      - Patch tokens raw: {patch_tokens_raw.shape}")
    print(f"      - Objectness map: {objectness_map.shape if objectness_map is not None else 'None'}")
    print(f"      - Attention map: {attention_map.shape if attention_map is not None else 'None'}")
    
    # ä¿å­˜ objectness map
    if objectness_map is not None:
        print(f"\nğŸ’¾ Saving objectness map...")
        save_heatmap(objectness_map, output_dir / "objectness", image_rgb)
    else:
        print(f"\nâš ï¸  No objectness map available")
    
    # ä¿å­˜ attention map
    if attention_map is not None:
        print(f"\nğŸ’¾ Saving attention map...")
        save_heatmap(attention_map, output_dir / "attention", image_rgb)
    else:
        print(f"\nâš ï¸  No attention map available")
    
    # å¦‚æœæŒ‡å®šè·³è¿‡ adaptersï¼Œåˆ™åœ¨æ­¤ç»“æŸ
    if args.skip_adapters:
        print(f"\nâ­ï¸  Skipping adapters as requested")
        print(f"\nâœ… Visualization complete!")
        print(f"   Output directory: {output_dir}")
        return
    
    # å¯¼å…¥ adapter æ¨¡å—
    from src.adapters.detection import build_detection_adapter, DetectionAdapterConfig
    from src.adapters.segmentation import build_segmentation_adapter, SegmentationAdapterConfig
    
    processed_hw = (processed_shape[1], processed_shape[0])
    
    # Detection Adapter
    detection_config_dict = model_section.get('detection_adapter', {})
    if detection_config_dict:
        print(f"\n============================================================")
        print(f"ğŸ¯ Step 2: Running Detection Adapter")
        print(f"============================================================")
        
        checkpoint_path = detection_config_dict.get('checkpoint_path', '')
        print(f"   Checkpoint: {checkpoint_path or '<random initialization>'}")
        print(f"   Threshold: {args.detection_threshold}")
        
        detection_config = DetectionAdapterConfig(
            checkpoint_path=checkpoint_path,
            feature_dim=patch_tokens_raw.shape[-1],
            num_classes=detection_config_dict.get('num_classes', 91),
            score_threshold=args.detection_threshold
        )
        
        detection_adapter = build_detection_adapter(
            detection_config,
            device=str(device),
            torch_dtype=dtype
        )
        
        print(f"   ğŸ”„ Running detection...")
        detection = detection_adapter.predict(
            patch_tokens_raw,
            image_size=processed_hw,
            grid_size=grid_size
        )
        
        print(f"   âœ… Detection complete: {len(detection.boxes)} boxes")
        if len(detection.boxes) > 0:
            print(f"      Score range: [{detection.scores.min():.3f}, {detection.scores.max():.3f}]")
            
            # ğŸ†• ä¿å­˜æ£€æµ‹å¯è§†åŒ–
            save_detection_visualization(detection, image_rgb, processed_shape, output_dir)
    
    # Segmentation Adapter
    segmentation_config_dict = model_section.get('segmentation_adapter', {})
    if segmentation_config_dict:
        print(f"\n============================================================")
        print(f"ğŸ–¼ï¸  Step 3: Running Segmentation Adapter")
        print(f"============================================================")
        
        checkpoint_path = segmentation_config_dict.get('checkpoint_path', '')
        print(f"   Checkpoint: {checkpoint_path or '<random initialization>'}")
        
        segmentation_config = SegmentationAdapterConfig(
            checkpoint_path=checkpoint_path,
            feature_dim=patch_tokens_raw.shape[-1],
            num_classes=segmentation_config_dict.get('num_classes', 150),
        )
        
        segmentation_adapter = build_segmentation_adapter(
            segmentation_config,
            device=str(device),
            torch_dtype=dtype
        )
        
        print(f"   ğŸ”„ Running segmentation...")
        segmentation = segmentation_adapter.predict(
            patch_tokens_raw,
            image_size=processed_hw,
            grid_size=grid_size
        )
        
        print(f"   âœ… Segmentation complete")
        print(f"      Logits shape: {segmentation.logits.shape}")
        print(f"      Logits range: [{segmentation.logits.min():.3f}, {segmentation.logits.max():.3f}]")
        
        # ğŸ†• ä¿å­˜åˆ†å‰²å¯è§†åŒ–
        save_segmentation_visualization(segmentation, image_rgb, processed_shape, output_dir)
    
    # ä¿å­˜å…ƒæ•°æ®
    import json
    metadata = {
        "image": args.input,
        "image_shape": list(image_rgb.shape),
        "processed_shape": list(processed_shape),
        "grid_size": list(grid_size),
        "detection_threshold": args.detection_threshold,
        "feature_dim": patch_tokens.shape[-1],
        "feature_dim_raw": patch_tokens_raw.shape[-1],
    }
    
    if 'detection' in locals():
        metadata["num_detections"] = len(detection.boxes)
    if 'segmentation' in locals():
        metadata["num_segmentation_classes"] = segmentation.logits.shape[0]
    
    with open(output_dir / "metadata.json", 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"\n" + "=" * 70)
    print(f"âœ… Visualization Complete!")
    print(f"=" * 70)
    print(f"\nğŸ“ Output directory: {output_dir}")
    print(f"\nğŸ“„ Generated files:")
    if objectness_map is not None:
        print(f"   - objectness.png / objectness_overlay.png")
    if attention_map is not None:
        print(f"   - attention.png / attention_overlay.png")
    if 'detection' in locals() and len(detection.boxes) > 0:
        print(f"   - detections.png ({len(detection.boxes)} boxes)")
    if 'segmentation' in locals():
        print(f"   - segmentation_map.png / segmentation_overlay.png")
        print(f"   - segmentation_foreground.png / segmentation_foreground_overlay.png")
    print(f"   - metadata.json")

if __name__ == "__main__":
    main()