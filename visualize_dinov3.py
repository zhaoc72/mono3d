#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆ DINOv3 å¯è§†åŒ–è„šæœ¬
ä¸“é—¨æ”¯æŒ ViT-7B/16 æ¨¡å‹å’Œæ­£ç¡®çš„è·¯å¾„é…ç½®
"""

import argparse
import sys
from pathlib import Path
import cv2
import numpy as np
import torch
import yaml

def load_yaml(path: str):
    """åŠ è½½ YAML é…ç½®æ–‡ä»¶"""
    with open(path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def normalize_to_uint8(data: np.ndarray) -> np.ndarray:
    """å°†æµ®ç‚¹æ•°ç»„å½’ä¸€åŒ–åˆ° uint8 èŒƒå›´ [0, 255]"""
    finite = np.isfinite(data)
    if not finite.any():
        return np.zeros_like(data, dtype=np.uint8)
    
    clipped = np.zeros_like(data, dtype=np.float16)
    clipped[finite] = data[finite]
    minimum = float(clipped[finite].min())
    maximum = float(clipped[finite].max())
    
    if maximum - minimum < 1e-6:
        return np.zeros_like(clipped, dtype=np.uint8)
    
    normalized = (clipped - minimum) / (maximum - minimum)
    normalized = np.clip(normalized, 0.0, 1.0)
    return (normalized * 255).astype(np.uint8)

def save_heatmap(map_2d: np.ndarray, output_path: Path, base_image: np.ndarray = None):
    """ä¿å­˜çƒ­åŠ›å›¾å¯è§†åŒ–"""
    heatmap_uint8 = normalize_to_uint8(map_2d)
    heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_VIRIDIS)
    
    # ä¿å­˜ç‹¬ç«‹çƒ­åŠ›å›¾
    cv2.imwrite(str(output_path.with_suffix('.png')), heatmap_color)
    print(f"   âœ… Saved: {output_path.name}")
    
    # å¦‚æœæä¾›äº†åŸºç¡€å›¾åƒï¼Œä¿å­˜å åŠ å›¾
    if base_image is not None:
        base_bgr = cv2.cvtColor(base_image, cv2.COLOR_RGB2BGR)
        # è°ƒæ•´çƒ­åŠ›å›¾å¤§å°ä»¥åŒ¹é…å›¾åƒ
        if heatmap_color.shape[:2] != base_bgr.shape[:2]:
            heatmap_color = cv2.resize(
                heatmap_color,
                (base_bgr.shape[1], base_bgr.shape[0]),
                interpolation=cv2.INTER_CUBIC
            )
        overlay = cv2.addWeighted(base_bgr, 0.6, heatmap_color, 0.4, 0.0)
        overlay_path = output_path.with_name(output_path.stem + '_overlay.png')
        cv2.imwrite(str(overlay_path), overlay)
        print(f"   âœ… Saved: {overlay_path.name}")

def main():
    parser = argparse.ArgumentParser(
        description="å¯è§†åŒ– DINOv3 backbone å’Œ adapter è¾“å‡º"
    )
    parser.add_argument("--input", required=True, help="è¾“å…¥å›¾åƒè·¯å¾„")
    parser.add_argument("--output", required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--config", required=True, help="é…ç½® YAML æ–‡ä»¶")
    parser.add_argument("--device", default="cuda", help="è®¾å¤‡ (cuda/cpu)")
    parser.add_argument(
        "--detection-threshold", type=float, default=0.05,
        help="æ£€æµ‹åˆ†æ•°é˜ˆå€¼ (é»˜è®¤: 0.05)"
    )
    parser.add_argument(
        "--skip-adapters", action="store_true",
        help="è·³è¿‡ adapterï¼Œåªè¿è¡Œ backbone"
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    print("=" * 70)
    print("DINOv3 Visualization (ViT-7B/16)")
    print("=" * 70)
    print(f"\nğŸ“– Loading config: {args.config}")
    
    config = load_yaml(args.config)
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½å›¾åƒ
    print(f"\nğŸ–¼ï¸  Loading image: {args.input}")
    image_bgr = cv2.imread(args.input)
    if image_bgr is None:
        raise ValueError(f"Failed to load image from {args.input}")
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    print(f"   Image shape: {image_rgb.shape}")
    
    # å¯¼å…¥å¿…éœ€æ¨¡å—
    print(f"\nğŸ“¦ Importing modules...")
    sys.path.insert(0, str(Path(__file__).parent))
    
    from src.config import Dinov3BackboneConfig
    from src.dinov3_feature import Dinov3Backbone
    
    # è®¾ç½®è®¾å¤‡å’Œæ•°æ®ç±»å‹
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    dtype = torch.float16
    print(f"   Device: {device}")
    print(f"   Dtype: {dtype}")
    
    # è§£æ DINOv3 backbone é…ç½®
    print(f"\nâš™ï¸  Parsing configuration...")
    model_section = config.get('model', config)
    backbone_config_dict = model_section.get('backbone', {})
    
    if not backbone_config_dict:
        raise ValueError("Could not find 'model.backbone' section in config")
    
    print(f"   Backbone config keys: {list(backbone_config_dict.keys())}")
    
    # åˆå§‹åŒ– DINOv3 backbone
    print(f"\n============================================================")
    print(f"ğŸ§  Step 1: Initializing DINOv3 Backbone")
    print(f"============================================================")
    
    repo_path = backbone_config_dict.get('repo_path')
    checkpoint_path = backbone_config_dict.get('checkpoint_path')
    
    print(f"   Repo path: {repo_path}")
    print(f"   Checkpoint: {checkpoint_path}")
    print(f"   Model name: {backbone_config_dict.get('model_name', 'dinov3_vit7b16')}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if repo_path and not Path(repo_path).exists():
        raise FileNotFoundError(f"DINOv3 repo not found: {repo_path}")
    if checkpoint_path and not Path(checkpoint_path).exists():
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    dinov3_config = Dinov3BackboneConfig(
        repo_path=repo_path,
        model_name=backbone_config_dict.get('model_name', 'dinov3_vit7b16'),
        checkpoint_path=checkpoint_path,
        image_size=backbone_config_dict.get('image_size', 518),
        output_layers=tuple(backbone_config_dict.get('output_layers', [9, 19, 29, 39])),
        enable_objectness=backbone_config_dict.get('enable_objectness', True),
        enable_pca=backbone_config_dict.get('enable_pca', True),
        pca_dim=backbone_config_dict.get('pca_dim', 32),
        patch_size=backbone_config_dict.get('patch_size', 16),
    )
    
    backbone = Dinov3Backbone(dinov3_config, device=device, dtype=dtype)
    print(f"   âœ… Backbone initialized successfully")
    
    # æå–ç‰¹å¾
    print(f"\nğŸ”„ Extracting DINOv3 features...")
    features = backbone.extract_features(image_rgb)
    
    patch_tokens = features['patch_tokens']
    patch_tokens_raw = features.get('patch_tokens_raw', patch_tokens)
    grid_size = features['grid_size']
    processed_shape = features['processed_image_shape']
    objectness_map = features.get('objectness_map')
    attention_map = features.get('attention_map')
    
    print(f"   âœ… Feature extraction complete:")
    print(f"      - Grid size: {grid_size}")
    print(f"      - Processed shape: {processed_shape}")
    print(f"      - Patch tokens: {patch_tokens.shape}")
    print(f"      - Patch tokens raw: {patch_tokens_raw.shape}")
    print(f"      - Objectness map: {objectness_map.shape if objectness_map is not None else 'None'}")
    print(f"      - Attention map: {attention_map.shape if attention_map is not None else 'None'}")
    
    # ä¿å­˜ objectness map
    if objectness_map is not None:
        print(f"\nğŸ’¾ Saving objectness map...")
        save_heatmap(objectness_map, output_dir / "objectness", image_rgb)
    else:
        print(f"\nâš ï¸  No objectness map available")
    
    # ä¿å­˜ attention map
    if attention_map is not None:
        print(f"\nğŸ’¾ Saving attention map...")
        save_heatmap(attention_map, output_dir / "attention", image_rgb)
    else:
        print(f"\nâš ï¸  No attention map available")
    
    # å¦‚æœæŒ‡å®šè·³è¿‡ adaptersï¼Œåˆ™åœ¨æ­¤ç»“æŸ
    if args.skip_adapters:
        print(f"\nâ­ï¸  Skipping adapters as requested")
        print(f"\nâœ… Visualization complete!")
        print(f"   Output directory: {output_dir}")
        return
    
    # å¯¼å…¥ adapter æ¨¡å—
    from src.adapters.detection import build_detection_adapter, DetectionAdapterConfig
    from src.adapters.segmentation import build_segmentation_adapter, SegmentationAdapterConfig
    
    processed_hw = (processed_shape[1], processed_shape[0])
    
    # Detection Adapter
    detection_config_dict = model_section.get('detection_adapter', {})
    if detection_config_dict:
        print(f"\n============================================================")
        print(f"ğŸ¯ Step 2: Running Detection Adapter")
        print(f"============================================================")
        
        checkpoint_path = detection_config_dict.get('checkpoint_path', '')
        print(f"   Checkpoint: {checkpoint_path or '<random initialization>'}")
        print(f"   Threshold: {args.detection_threshold}")
        
        detection_config = DetectionAdapterConfig(
            checkpoint_path=checkpoint_path,
            feature_dim=patch_tokens_raw.shape[-1],
            num_classes=detection_config_dict.get('num_classes', 91),
            score_threshold=args.detection_threshold
        )
        
        detection_adapter = build_detection_adapter(
            detection_config,
            device=str(device),
            torch_dtype=dtype
        )
        
        print(f"   ğŸ”„ Running detection...")
        detection = detection_adapter.predict(
            patch_tokens_raw,
            image_size=processed_hw,
            grid_size=grid_size
        )
        
        print(f"   âœ… Detection complete: {len(detection.boxes)} boxes")
        if len(detection.boxes) > 0:
            print(f"      Score range: [{detection.scores.min():.3f}, {detection.scores.max():.3f}]")
    
    # Segmentation Adapter
    segmentation_config_dict = model_section.get('segmentation_adapter', {})
    if segmentation_config_dict:
        print(f"\n============================================================")
        print(f"ğŸ–¼ï¸  Step 3: Running Segmentation Adapter")
        print(f"============================================================")
        
        checkpoint_path = segmentation_config_dict.get('checkpoint_path', '')
        print(f"   Checkpoint: {checkpoint_path or '<random initialization>'}")
        
        segmentation_config = SegmentationAdapterConfig(
            checkpoint_path=checkpoint_path,
            feature_dim=patch_tokens_raw.shape[-1],
            num_classes=segmentation_config_dict.get('num_classes', 150),
        )
        
        segmentation_adapter = build_segmentation_adapter(
            segmentation_config,
            device=str(device),
            torch_dtype=dtype
        )
        
        print(f"   ğŸ”„ Running segmentation...")
        segmentation = segmentation_adapter.predict(
            patch_tokens_raw,
            image_size=processed_hw,
            grid_size=grid_size
        )
        
        print(f"   âœ… Segmentation complete")
        print(f"      Logits shape: {segmentation.logits.shape}")
        print(f"      Logits range: [{segmentation.logits.min():.3f}, {segmentation.logits.max():.3f}]")
    
    # ä¿å­˜å…ƒæ•°æ®
    import json
    metadata = {
        "image": args.input,
        "image_shape": list(image_rgb.shape),
        "processed_shape": list(processed_shape),
        "grid_size": list(grid_size),
        "detection_threshold": args.detection_threshold,
        "feature_dim": patch_tokens.shape[-1],
        "feature_dim_raw": patch_tokens_raw.shape[-1],
    }
    
    if 'detection' in locals():
        metadata["num_detections"] = len(detection.boxes)
    if 'segmentation' in locals():
        metadata["num_segmentation_classes"] = segmentation.logits.shape[0]
    
    with open(output_dir / "metadata.json", 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"\n" + "=" * 70)
    print(f"âœ… Visualization Complete!")
    print(f"=" * 70)
    print(f"\nğŸ“ Output directory: {output_dir}")
    print(f"\nğŸ“„ Generated files:")
    if objectness_map is not None:
        print(f"   - objectness.png / objectness_overlay.png")
    if attention_map is not None:
        print(f"   - attention.png / attention_overlay.png")
    if 'detection' in locals():
        print(f"   - Detection: {len(detection.boxes)} boxes")
    if 'segmentation' in locals():
        print(f"   - Segmentation: {segmentation.logits.shape[0]} classes")
    print(f"   - metadata.json")

if __name__ == "__main__":
    main()