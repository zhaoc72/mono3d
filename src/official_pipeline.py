"""ä½¿ç”¨å®˜æ–¹DINOv3ä»£ç çš„é›¶æ ·æœ¬å®ä¾‹åˆ†å‰²ç®¡é“"""
from __future__ import annotations

import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
from torchvision import transforms as v2
from PIL import Image

from ..utils import LOGGER


class OfficialDINOv3Pipeline:
    """ä½¿ç”¨å®˜æ–¹DINOv3ä»£ç çš„å®Œæ•´ç®¡é“"""
    
    def __init__(
        self,
        config: Dict[str, Any],
        device: Optional[str] = None,
    ):
        self.config = config
        self.official_config = config['dinov3_official']
        self.repo_dir = self.official_config['repo_dir']
        
        # æ·»åŠ å®˜æ–¹ä»£ç è·¯å¾„åˆ°Pythonè·¯å¾„
        if self.repo_dir not in sys.path:
            sys.path.insert(0, self.repo_dir)
        
        # å¤šGPUé…ç½®
        self.multi_gpu_config = self.official_config.get('multi_gpu', {})
        self.use_multi_gpu = self.multi_gpu_config.get('enabled', True)
        self.num_gpus = torch.cuda.device_count()
        
        if self.use_multi_gpu and self.num_gpus >= 2:
            LOGGER.info(f"ğŸš€ Multi-GPU enabled: {self.num_gpus} GPUs detected")
            self.device = torch.device("cuda:0")  # ä¸»è®¾å¤‡
        else:
            self.device = torch.device(device or "cuda")
            LOGGER.info(f"ğŸ“Œ Single GPU mode: {self.device}")
        
        # æ¨ç†é…ç½®
        self.inference_config = self.official_config['inference']
        self.image_size = self.inference_config['image_size']
        self.dtype = self._resolve_dtype(self.inference_config['dtype'])
        
        # åŠ è½½æ¨¡å‹
        self._load_models()
        
        # å›¾åƒé¢„å¤„ç†
        self.transform = self._make_transform(self.image_size)
    
    @staticmethod
    def _resolve_dtype(dtype_str: str) -> torch.dtype:
        """è§£ædtypeå­—ç¬¦ä¸²"""
        lookup = {
            "float16": torch.float16,
            "float32": torch.float32,
            "bfloat16": torch.bfloat16,
        }
        return lookup.get(dtype_str.lower(), torch.bfloat16)
    
    def _make_transform(self, resize_size: int) -> v2.Compose:
        """åˆ›å»ºå›¾åƒé¢„å¤„ç†pipeline"""
        to_tensor = v2.ToImage()
        resize = v2.Resize((resize_size, resize_size), antialias=True)
        to_float = v2.ToDtype(torch.float32, scale=True)
        normalize = v2.Normalize(
            mean=(0.485, 0.456, 0.406),
            std=(0.229, 0.224, 0.225),
        )
        return v2.Compose([to_tensor, resize, to_float, normalize])
    
    def _load_with_model_parallel(
        self,
        model: nn.Module,
        model_name: str,
    ) -> nn.Module:
        """ä½¿ç”¨æ¨¡å‹å¹¶è¡ŒåŠ è½½å¤§æ¨¡å‹"""
        try:
            from accelerate import (
                init_empty_weights,
                load_checkpoint_and_dispatch,
                infer_auto_device_map,
            )
        except ImportError:
            raise ImportError("è¯·å®‰è£… accelerate: pip install accelerate")
        
        LOGGER.info(f"ğŸ”„ Loading {model_name} with model parallelism...")
        
        # é…ç½®æ˜¾å­˜åˆ†é…
        max_memory = self.multi_gpu_config.get('max_memory', {
            0: "18GiB",
            1: "18GiB",
            2: "18GiB",
            3: "18GiB",
            "cpu": "100GiB",
        })
        
        # æ¨æ–­è®¾å¤‡æ˜ å°„
        device_map = infer_auto_device_map(
            model,
            max_memory=max_memory,
            no_split_module_classes=["Block"],  # ä¸åˆ‡åˆ†Transformerå—
            dtype=self.dtype,
        )
        
        # æ‰“å°è®¾å¤‡åˆ†é…
        device_stats = {}
        for key, device in device_map.items():
            device_stats[device] = device_stats.get(device, 0) + 1
        
        LOGGER.info(f"ğŸ“Š {model_name} device allocation:")
        for device in sorted(device_stats.keys()):
            count = device_stats[device]
            LOGGER.info(f"   - {device}: {count} modules")
        
        return model
    
    def _load_models(self):
        """åŠ è½½æ‰€æœ‰æ¨¡å‹"""
        LOGGER.info("=" * 70)
        LOGGER.info("ğŸ§  Loading DINOv3 Official Models")
        LOGGER.info("=" * 70)
        
        backbone_cfg = self.official_config['backbone']
        detection_cfg = self.official_config['detection']
        segmentation_cfg = self.official_config['segmentation']
        
        # åŠ è½½æ£€æµ‹æ¨¡å‹
        LOGGER.info(f"\nğŸ¯ Loading detector: {detection_cfg['model_name']}")
        LOGGER.info(f"   Weights: {detection_cfg['checkpoint_path']}")
        LOGGER.info(f"   Backbone: {backbone_cfg['checkpoint_path']}")
        
        self.detector = torch.hub.load(
            self.repo_dir,
            detection_cfg['model_name'],
            source="local",
            weights=detection_cfg['checkpoint_path'],
            backbone_weights=backbone_cfg['checkpoint_path'],
            trust_repo=True,
        )
        
        if self.use_multi_gpu and self.num_gpus >= 2:
            self.detector = self._load_with_model_parallel(
                self.detector, "Detector"
            )
        else:
            self.detector = self.detector.to(self.device, dtype=self.dtype)
        
        self.detector.eval()
        LOGGER.info("   âœ… Detector loaded")
        
        # åŠ è½½åˆ†å‰²æ¨¡å‹
        LOGGER.info(f"\nğŸ–¼ï¸  Loading segmentor: {segmentation_cfg['model_name']}")
        LOGGER.info(f"   Weights: {segmentation_cfg['checkpoint_path']}")
        LOGGER.info(f"   Backbone: {backbone_cfg['checkpoint_path']}")
        
        self.segmentor = torch.hub.load(
            self.repo_dir,
            segmentation_cfg['model_name'],
            source="local",
            weights=segmentation_cfg['checkpoint_path'],
            backbone_weights=backbone_cfg['checkpoint_path'],
            trust_repo=True,
        )
        
        if self.use_multi_gpu and self.num_gpus >= 2:
            self.segmentor = self._load_with_model_parallel(
                self.segmentor, "Segmentor"
            )
        else:
            self.segmentor = self.segmentor.to(self.device, dtype=self.dtype)
        
        self.segmentor.eval()
        LOGGER.info("   âœ… Segmentor loaded")
        
        # æ‰“å°æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            LOGGER.info("\nğŸ¯ GPU Memory Status:")
            for i in range(min(self.num_gpus, 4)):
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                reserved = torch.cuda.memory_reserved(i) / 1024**3
                LOGGER.info(
                    f"   GPU {i}: {allocated:.2f}GB allocated, "
                    f"{reserved:.2f}GB reserved"
                )
        
        LOGGER.info("=" * 70)
        LOGGER.info("âœ… All models loaded successfully")
        LOGGER.info("=" * 70)
    
    def _preprocess_image(self, image: np.ndarray) -> torch.Tensor:
        """é¢„å¤„ç†å›¾åƒ"""
        # è½¬æ¢ä¸ºPIL Image
        if isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image)
        else:
            pil_image = image
        
        # åº”ç”¨transform
        tensor = self.transform(pil_image)
        return tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
    
    @torch.inference_mode()
    def run_detection(
        self,
        image: np.ndarray,
    ) -> Dict[str, Any]:
        """è¿è¡Œæ£€æµ‹"""
        LOGGER.info("ğŸ¯ Running detection...")
        
        # é¢„å¤„ç†
        batch_img = self._preprocess_image(image).to(self.device)
        
        # æ¨ç†
        with torch.autocast('cuda', dtype=self.dtype):
            predictions = self.detector(batch_img)
        
        # è§£æç»“æœ
        # DINOv3 detectorè¿”å›æ ¼å¼ï¼š[{'boxes': ..., 'scores': ..., 'labels': ...}]
        pred = predictions[0] if isinstance(predictions, list) else predictions
        
        boxes = pred['boxes'].cpu().numpy()
        scores = pred['scores'].cpu().numpy()
        labels = pred['labels'].cpu().numpy()
        
        # è¿‡æ»¤ä½åˆ†æ£€æµ‹
        threshold = self.official_config['detection']['score_threshold']
        valid_mask = scores >= threshold
        
        result = {
            'boxes': boxes[valid_mask],
            'scores': scores[valid_mask],
            'labels': labels[valid_mask],
            'num_detections': int(valid_mask.sum()),
        }
        
        LOGGER.info(f"   âœ… Detected {result['num_detections']} objects")
        return result
    
    @torch.inference_mode()
    def run_segmentation(
        self,
        image: np.ndarray,
    ) -> Dict[str, Any]:
        """è¿è¡Œåˆ†å‰²"""
        LOGGER.info("ğŸ–¼ï¸  Running segmentation...")
        
        # å¯¼å…¥åˆ†å‰²æ¨ç†å·¥å…·
        from dinov3.eval.segmentation.inference import make_inference
        from functools import partial
        
        seg_cfg = self.official_config['segmentation']
        inf_cfg = self.inference_config
        
        # é¢„å¤„ç†
        pil_image = Image.fromarray(image) if isinstance(image, np.ndarray) else image
        original_size = pil_image.size  # (W, H)
        
        batch_img = self._preprocess_image(image).to(self.device)
        
        # æ»‘çª—æ¨ç†ï¼ˆå¤„ç†å¤§åˆ†è¾¨ç‡å›¾åƒï¼‰
        with torch.autocast('cuda', dtype=self.dtype):
            segmentation_map = make_inference(
                batch_img,
                self.segmentor,
                inference_mode=seg_cfg.get('inference_mode', 'slide'),
                decoder_head_type=seg_cfg.get('decoder_head_type', 'm2f'),
                rescale_to=original_size,
                n_output_channels=seg_cfg['num_classes'],
                crop_size=tuple(inf_cfg['crop_size']),
                stride=tuple(inf_cfg['stride']),
                output_activation=partial(torch.nn.functional.softmax, dim=1),
            )
        
        # è½¬æ¢ä¸ºnumpy
        # segmentation_map: [B, C, H, W]
        probs = segmentation_map[0].cpu().numpy()  # [C, H, W]
        class_map = segmentation_map.argmax(dim=1, keepdim=False)[0].cpu().numpy()  # [H, W]
        
        result = {
            'probs': probs,  # [C, H, W] æ¦‚ç‡å›¾
            'class_map': class_map,  # [H, W] ç±»åˆ«ç´¢å¼•
            'num_classes': seg_cfg['num_classes'],
        }
        
        LOGGER.info(f"   âœ… Segmentation complete: {result['num_classes']} classes")
        return result
    
    def run(self, image: np.ndarray) -> Dict[str, Any]:
        """å®Œæ•´æ¨ç†ç®¡é“"""
        LOGGER.info("\n" + "=" * 70)
        LOGGER.info("ğŸš€ Running Official DINOv3 Pipeline")
        LOGGER.info("=" * 70)
        
        # è¿è¡Œæ£€æµ‹
        detection_result = self.run_detection(image)
        
        # è¿è¡Œåˆ†å‰²
        segmentation_result = self.run_segmentation(image)
        
        # åˆå¹¶ç»“æœ
        result = {
            'detection': detection_result,
            'segmentation': segmentation_result,
            'image_shape': image.shape[:2],
            'processed_size': (self.image_size, self.image_size),
        }
        
        LOGGER.info("=" * 70)
        LOGGER.info("âœ… Pipeline complete")
        LOGGER.info("=" * 70)
        
        return result