#!/usr/bin/env python3
"""
å¯¹æ¯”ä¸åŒçš„çƒ­å›¾ç”Ÿæˆæ–¹æ³•å¯¹SAM2åˆ†å‰²æ•ˆæœçš„å½±å“

æµ‹è¯•æ–¹æ³•:
1. ç‰©ä½“æ€§è¯„åˆ†æ–¹æ³• (Objectness)
2. ç»„åˆæ–¹æ³• (Combined: objectness + anomaly + contrast)
3. å½“å‰çš„Attentionæ–¹æ³• (ä½œä¸ºbaseline)
"""

import sys
import yaml
import torch
import numpy as np
import cv2
from pathlib import Path
import torch.nn.functional as F

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/media/pc/D/zhaochen/mono3d/mono3d')

from src.dinov3_feature import DINOv3FeatureExtractor, Dinov3Config
from src.sam2_segmenter import SAM2Segmenter, Sam2Config
from src.prompt_generator import PromptConfig, generate_prompts_from_heatmap
from src.data_loader import load_image
from src.utils import to_torch_dtype

print("=" * 80)
print("DINOv3çƒ­å›¾æ–¹æ³• + SAM2åˆ†å‰²æ•ˆæœå¯¹æ¯”")
print("=" * 80)


class HeatmapGenerator:
    """ä¸åŒçš„çƒ­å›¾ç”Ÿæˆæ–¹æ³•"""
    
    def __init__(self, model, device):
        self.model = model
        self.device = device
    
    def extract_features(self, image):
        """æå–DINOv3ç‰¹å¾"""
        from torchvision import transforms
        
        transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((518, 518)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])
        
        image_tensor = transform(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            features_dict = self.model.forward_features(image_tensor)
            patch_features = features_dict['x_norm_patchtokens'][0]  # [N, D]
        
        H = W = int(np.sqrt(len(patch_features)))
        return patch_features, H, W
    
    def method_objectness(self, image):
        """æ–¹æ³•1: ç‰©ä½“æ€§è¯„åˆ†"""
        patch_features, H, W = self.extract_features(image)
        
        # å½’ä¸€åŒ–ç‰¹å¾
        patch_features_norm = F.normalize(patch_features, dim=-1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.mm(patch_features_norm, patch_features_norm.t())
        
        # ç‰©ä½“æ€§ = 1 - ä¸é‚»è¿‘patchesçš„å¹³å‡ç›¸ä¼¼åº¦
        K = 20
        topk_sim, _ = torch.topk(similarity_matrix, k=K+1, dim=1, largest=True)
        avg_similarity = topk_sim[:, 1:].mean(dim=1)
        objectness = (1 - avg_similarity).cpu().numpy()
        
        # é‡å¡‘ä¸º2D
        objectness_2d = objectness[:H*W].reshape(H, W)
        
        return objectness_2d, "Objectness"
    
    def method_combined(self, image):
        """æ–¹æ³•2: ç»„åˆæ–¹æ³•"""
        patch_features, H, W = self.extract_features(image)
        
        # 1. ç‰©ä½“æ€§
        patch_features_norm = F.normalize(patch_features, dim=-1)
        similarity_matrix = torch.mm(patch_features_norm, patch_features_norm.t())
        K = 20
        topk_sim, _ = torch.topk(similarity_matrix, k=K+1, dim=1, largest=True)
        avg_similarity = topk_sim[:, 1:].mean(dim=1)
        objectness = (1 - avg_similarity).cpu().numpy()
        objectness_2d = objectness[:H*W].reshape(H, W)
        
        # 2. å¼‚å¸¸åº¦
        mean_feature = patch_features.mean(dim=0, keepdim=True)
        distances = torch.norm(patch_features - mean_feature, dim=-1).cpu().numpy()
        anomaly_2d = distances[:H*W].reshape(H, W)
        
        # 3. å±€éƒ¨å¯¹æ¯”åº¦
        feature_map = patch_features.reshape(H, W, -1)
        contrast = torch.zeros(H, W, device=self.device)
        
        for di in [-1, 0, 1]:
            for dj in [-1, 0, 1]:
                if di == 0 and dj == 0:
                    continue
                
                i_start = max(0, -di)
                i_end = H + min(0, -di)
                j_start = max(0, -dj)
                j_end = W + min(0, -dj)
                
                center = feature_map[i_start:i_end, j_start:j_end, :]
                neighbor = feature_map[i_start+di:i_end+di, j_start+dj:j_end+dj, :]
                
                diff = torch.norm(center - neighbor, dim=-1)
                contrast[i_start:i_end, j_start:j_end] += diff
        
        contrast = contrast.cpu().numpy()
        
        # å½’ä¸€åŒ–å¹¶ç»„åˆ
        objectness_norm = (objectness_2d - objectness_2d.min()) / (objectness_2d.max() - objectness_2d.min() + 1e-8)
        anomaly_norm = (anomaly_2d - anomaly_2d.min()) / (anomaly_2d.max() - anomaly_2d.min() + 1e-8)
        contrast_norm = (contrast - contrast.min()) / (contrast.max() - contrast.min() + 1e-8)
        
        combined = (
            objectness_norm * 0.4 +
            anomaly_norm * 0.3 +
            contrast_norm * 0.3
        )
        
        return combined, "Combined"
    
    def method_attention(self, image, extractor):
        """æ–¹æ³•3: å½“å‰çš„Attentionæ–¹æ³• (baseline)"""
        feats = extractor.extract(image)
        attention = feats["attention"]
        
        if attention is None:
            raise ValueError("Failed to extract attention")
        
        # ä½¿ç”¨ç°æœ‰çš„attention_to_heatmapæ–¹æ³•
        heatmap = extractor.attention_to_heatmap(
            attention,
            (image.shape[1], image.shape[0])
        )
        
        return heatmap, "Attention (Current)"


def resize_heatmap(heatmap, target_size):
    """è°ƒæ•´çƒ­å›¾å¤§å°åˆ°ç›®æ ‡å°ºå¯¸"""
    return cv2.resize(
        heatmap,
        target_size,
        interpolation=cv2.INTER_CUBIC
    )


def save_heatmap_visualization(heatmap, image, name, output_dir):
    """ä¿å­˜çƒ­å›¾å¯è§†åŒ–"""
    # å½’ä¸€åŒ–
    heatmap_norm = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)
    
    # çƒ­å›¾
    heatmap_vis = (heatmap_norm * 255).astype(np.uint8)
    heatmap_colored = cv2.applyColorMap(heatmap_vis, cv2.COLORMAP_JET)
    cv2.imwrite(str(output_dir / f"{name}_heatmap.png"), heatmap_colored)
    
    # å åŠ 
    overlay = cv2.addWeighted(
        cv2.cvtColor(image, cv2.COLOR_RGB2BGR), 0.6,
        heatmap_colored, 0.4, 0
    )
    cv2.imwrite(str(output_dir / f"{name}_overlay.png"), overlay)


def visualize_boxes(image, boxes, name, output_dir):
    """å¯è§†åŒ–bounding boxes"""
    img_with_boxes = image.copy()
    
    for i, box in enumerate(boxes):
        x0, y0, x1, y1 = box
        cv2.rectangle(img_with_boxes, (x0, y0), (x1, y1), (0, 255, 0), 2)
        cv2.putText(
            img_with_boxes, f"{i}", (x0, y0-5),
            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2
        )
    
    cv2.imwrite(
        str(output_dir / f"{name}_boxes.png"),
        cv2.cvtColor(img_with_boxes, cv2.COLOR_RGB2BGR)
    )


def visualize_masks(image, masks, name, output_dir):
    """å¯è§†åŒ–åˆ†å‰²masks"""
    if not masks:
        print(f"  âš ï¸ {name}: æ²¡æœ‰ç”Ÿæˆä»»ä½•mask")
        return
    
    # å•ç‹¬ä¿å­˜æ¯ä¸ªmask
    combined = np.zeros_like(image)
    
    for i, mask in enumerate(masks):
        # ä¸ºæ¯ä¸ªmaskåˆ†é…ä¸åŒçš„é¢œè‰²
        color = np.array([
            (i * 50) % 255,
            (i * 80) % 255,
            (i * 120) % 255
        ], dtype=np.uint8)
        
        combined[mask.astype(bool)] = color
        
        # ä¿å­˜å‰3ä¸ªmask
        if i < 3:
            mask_vis = (mask.astype(np.uint8) * 255)
            cv2.imwrite(str(output_dir / f"{name}_mask_{i}.png"), mask_vis)
    
    # ä¿å­˜ç»„åˆçš„å¯è§†åŒ–
    cv2.imwrite(
        str(output_dir / f"{name}_all_masks.png"),
        cv2.cvtColor(combined, cv2.COLOR_RGB2BGR)
    )
    
    # å åŠ åˆ°åŸå›¾
    overlay = cv2.addWeighted(
        cv2.cvtColor(image, cv2.COLOR_RGB2BGR), 0.6,
        cv2.cvtColor(combined, cv2.COLOR_RGB2BGR), 0.4, 0
    )
    cv2.imwrite(str(output_dir / f"{name}_overlay.png"), overlay)


def analyze_results(method_name, heatmap, boxes, masks, area_threshold):
    """åˆ†æç»“æœ"""
    print(f"\n{'='*60}")
    print(f"{method_name}")
    print(f"{'='*60}")
    
    print(f"çƒ­å›¾ç»Ÿè®¡:")
    print(f"  èŒƒå›´: [{heatmap.min():.3f}, {heatmap.max():.3f}]")
    print(f"  å‡å€¼: {heatmap.mean():.3f}")
    print(f"  æ ‡å‡†å·®: {heatmap.std():.3f}")
    
    print(f"\nPromptsç”Ÿæˆ:")
    print(f"  Boxæ•°é‡: {len(boxes)}")
    
    if boxes:
        print(f"  å‰5ä¸ªboxes:")
        for i, box in enumerate(boxes[:5]):
            x0, y0, x1, y1 = box
            area = (x1 - x0) * (y1 - y0)
            print(f"    Box {i}: [{x0:4d}, {y0:4d}, {x1:4d}, {y1:4d}], area={area:6.0f}")
    
    print(f"\nSAM2åˆ†å‰²ç»“æœ:")
    print(f"  ç”Ÿæˆçš„maskæ•°é‡: {len(masks)}")
    
    if masks:
        valid_count = 0
        total_area = 0
        max_area = 0
        
        for i, mask in enumerate(masks):
            area = mask.sum()
            total_area += area
            max_area = max(max_area, area)
            
            passed = area >= area_threshold
            if passed:
                valid_count += 1
            
            status = "âœ…" if passed else "âŒ"
            if i < 10:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"    Mask {i}: area={area:8.0f} {status}")
        
        print(f"\n  é€šè¿‡é˜ˆå€¼çš„mask: {valid_count}/{len(masks)}")
        print(f"  å¹³å‡maské¢ç§¯: {total_area/len(masks):.0f}")
        print(f"  æœ€å¤§maské¢ç§¯: {max_area:.0f}")
        
        return {
            'total_masks': len(masks),
            'valid_masks': valid_count,
            'avg_area': total_area / len(masks),
            'max_area': max_area
        }
    else:
        return {
            'total_masks': 0,
            'valid_masks': 0,
            'avg_area': 0,
            'max_area': 0
        }


def main():
    # é…ç½®
    image_path = "/media/pc/D/datasets/vkitti2/Scene01/clone/frames/rgb/Camera_0/rgb_00000.jpg"
    config_path = "/media/pc/D/zhaochen/mono3d/mono3d/configs/model_config.yaml"
    prompt_config_path = "/media/pc/D/zhaochen/mono3d/mono3d/configs/prompt_config.yaml"
    
    output_dir = Path("/media/pc/D/zhaochen/mono3d/mono3d/outputs/heatmap_comparison")
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # åŠ è½½å›¾åƒ
    print(f"\nåŠ è½½å›¾åƒ: {image_path}")
    sample = load_image(image_path)
    print(f"å›¾åƒå½¢çŠ¶: {sample.image.shape}")
    
    # ä¿å­˜åŸå›¾
    cv2.imwrite(
        str(output_dir / "original.png"),
        cv2.cvtColor(sample.image, cv2.COLOR_RGB2BGR)
    )
    
    # åŠ è½½é…ç½®
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    with open(prompt_config_path, 'r') as f:
        prompt_cfg_dict = yaml.safe_load(f)
    
    device = torch.device(config["device"])
    dtype = to_torch_dtype(config["dtype"])
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("\nåˆå§‹åŒ–æ¨¡å‹...")
    dinov3_cfg = Dinov3Config(**config["dinov3"])
    extractor = DINOv3FeatureExtractor(dinov3_cfg, device, dtype)
    print("âœ… DINOv3åŠ è½½æˆåŠŸ")
    
    sam2_cfg = Sam2Config(**config["sam2"])
    segmenter = SAM2Segmenter(sam2_cfg, device, dtype)
    print("âœ… SAM2åŠ è½½æˆåŠŸ")
    
    # åˆå§‹åŒ–çƒ­å›¾ç”Ÿæˆå™¨
    heatmap_generator = HeatmapGenerator(extractor.model, device)
    
    # Prompté…ç½®
    prompt_params = {}
    if "attention" in prompt_cfg_dict:
        prompt_params.update(prompt_cfg_dict["attention"])
    if "points" in prompt_cfg_dict:
        prompt_params.update(prompt_cfg_dict["points"])
    prompt_config = PromptConfig(**prompt_params)
    
    area_threshold = config["pipeline"]["area_threshold"]
    
    print(f"\nPrompté…ç½®:")
    print(f"  percentile: {prompt_config.percentile}")
    print(f"  min_component_area: {prompt_config.min_component_area}")
    print(f"  area_threshold: {area_threshold}")
    
    # æµ‹è¯•ä¸‰ç§æ–¹æ³•
    methods = [
        ("objectness", heatmap_generator.method_objectness, sample.image),
        ("combined", heatmap_generator.method_combined, sample.image),
        ("attention", heatmap_generator.method_attention, sample.image, extractor),
    ]
    
    results_summary = {}
    
    for method_info in methods:
        if len(method_info) == 3:
            method_id, method_func, image = method_info
            heatmap_2d, method_name = method_func(image)
        else:
            method_id, method_func, image, extra_arg = method_info
            heatmap_2d, method_name = method_func(image, extra_arg)
        
        print(f"\n{'='*80}")
        print(f"æµ‹è¯•æ–¹æ³•: {method_name}")
        print(f"{'='*80}")
        
        # è°ƒæ•´çƒ­å›¾å¤§å°
        heatmap = resize_heatmap(heatmap_2d, (sample.image.shape[1], sample.image.shape[0]))
        
        # ä¿å­˜çƒ­å›¾å¯è§†åŒ–
        save_heatmap_visualization(heatmap, sample.image, method_id, output_dir)
        
        # ç”Ÿæˆprompts
        boxes, points, labels = generate_prompts_from_heatmap(heatmap, prompt_config)
        
        # å¯è§†åŒ–boxes
        if boxes:
            visualize_boxes(sample.image, boxes, method_id, output_dir)
        
        # è¿è¡ŒSAM2
        if boxes:
            masks = segmenter.segment_batched(
                sample.image,
                boxes,
                points=points if points else None,
                labels=labels if labels else None,
                batch_size=32,
            )
            
            # å¯è§†åŒ–masks
            visualize_masks(sample.image, masks, method_id, output_dir)
        else:
            masks = []
        
        # åˆ†æç»“æœ
        stats = analyze_results(method_name, heatmap, boxes, masks, area_threshold)
        results_summary[method_name] = stats
    
    # æ‰“å°æ€»ç»“
    print(f"\n{'='*80}")
    print("æ€»ç»“å¯¹æ¯”")
    print(f"{'='*80}")
    
    print(f"\n{'æ–¹æ³•':<25} {'æ€»Masks':<10} {'æœ‰æ•ˆMasks':<12} {'å¹³å‡é¢ç§¯':<12} {'æœ€å¤§é¢ç§¯':<12}")
    print("-" * 80)
    
    for method_name, stats in results_summary.items():
        print(f"{method_name:<25} {stats['total_masks']:<10} {stats['valid_masks']:<12} "
              f"{stats['avg_area']:<12.0f} {stats['max_area']:<12.0f}")
    
    # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
    best_method = max(results_summary.items(), key=lambda x: x[1]['valid_masks'])
    
    print(f"\n{'='*80}")
    print(f"ğŸ† æœ€ä½³æ–¹æ³•: {best_method[0]}")
    print(f"   æœ‰æ•ˆMasksæ•°é‡: {best_method[1]['valid_masks']}")
    print(f"{'='*80}")
    
    print(f"\næ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}/")
    print("\nå…³é”®æ–‡ä»¶:")
    print("  çƒ­å›¾å¯¹æ¯”:")
    print("    - objectness_overlay.png")
    print("    - combined_overlay.png")
    print("    - attention_overlay.png")
    print("\n  Boxeså¯¹æ¯”:")
    print("    - objectness_boxes.png")
    print("    - combined_boxes.png")
    print("    - attention_boxes.png")
    print("\n  åˆ†å‰²ç»“æœå¯¹æ¯”:")
    print("    - objectness_overlay.png")
    print("    - combined_overlay.png")
    print("    - attention_overlay.png")


if __name__ == "__main__":
    main()